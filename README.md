# Emotion_recognition_task
This is a pilot study with a limited trial set that adapts the stroop task by using facial and tone based expression of emotion. The participant is presented with a face (male/female) along with an audio (mix of words and non-words) that convey any of the following emotions - happiness, anger, sadness, fear and disgust. The participant uses the keyboard to identify which emotion the voice is expressing regardless of the emotion they can see on the person's face. 

`Aim`- this experiment allows the researcher to study the effects of multimodal interference in emotion recognition tasks. The data collected from this experiement can be used to analyse the role of auditory and visual stimuli in emotion recognition. 

`Dependencies` - Psychopy, Pandas

`Conditions.xlsx` - list of trials.

`sub-100.csv` - demo file that displays the end result of the experiment. This includes the reaction time, keyboard response and whether the particpant chose the correct answer per trial. The code also summarises the mean reaction time and total correct choices based on the two conditions (congruent and incongruent) at the end of the experiment. 
